\section{The POP Process}
\label{sec:popp}

The partially observable Poisson process (POPP) is a counting process $N(t)$ with arrival rate $\lambda$ where the number of events that occurred up to time $t$ are obtained by unreliable (possibly multiple) counters. The definition brings a distinction between \emph{true count} (or simply \emph{count}), which refers to the number of events that actually happened, and the \emph{sensed count}, which refers to the count obtained by a counter (or sensor). Given that $c_i$ number of events happened (as the true count) over the interval $[0, t)$ during the $i$-th observation, and $m$ counters observed the events unreliably, thus the sensed count $s_{ji}$ is the count given by sensor $j$ in the $i$-th observation within the interval $[0, t)$ with $0 \leq j \leq m$. 

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
	\tikzstyle{place}=[rectangle,draw=blue,thick,minimum size=5 mm]
	\tikzstyle{every label}=[black]
	\begin{scope}
	\node[place](31)[xshift=15mm]{$S_{1i}$};
	\node[place](32)[right of=31, xshift=7mm]{$S_{2i}$};
	\node[place](33)[right of=32, xshift=5mm]{$\ldots$};
	\node[place](34)[right of=33, xshift=7mm]{$S_{(m-1)i}$};
	\node[place](35)[right of=34, xshift=12mm]{$S_{mi}$};
	\node[place](21)[above of=33]{$C_i$} edge[post](31) edge[post](32) edge[post](33) edge[post](34) edge[post](35);
	\node[place](11)[above of=21]{$\lambda$} edge[post](21);
	% \node[place](01)[above of=11]{$\alpha, \beta$} edge[post](11);
	\end{scope}
	\end{tikzpicture}
    \caption{Graphical representation of the partially observable Poisson process.}
	\label{fig:gm_popp}
\end{figure}

The graphical model, which is easily derived from the definition of the POPP, shows that the true count $c_i$ has become a latent variable which can only be inferred from the sensed count $\overrightarrow{s_i} = (s_{1i}, \ldots, s_{mi})$ where each $s_{ji}$ is a sensed count coming from sensor $j$. The posterior of $\lambda$ is then inferred from the posterior of $x_i$ after multiple samples $i = 1 \ldots n$.

A statistical inference to estimate the rate parameter $\lambda$ of the POPP model is done with a marginalisation over all possible true count value $c_i$ in a joint distribution between the posterior $P(\lambda ; c_i)$ and the posterior over $c_i$ given $\overrightarrow{s_i}$. The posterior of $\lambda$, given $n$ samples $\overrightarrow{s}=(\overrightarrow{s_1} \dots \overrightarrow{s_n})$, each consisting of $m$ sensors, is:
\begin{equation}
	\label{eq:marginal_occurrences}
	\begin{tabular}{r@{=}l}
		$P(\lambda ; \overrightarrow{s})$ &  $\displaystyle\sum_{c_1=0}^{\infty} \ldots \displaystyle\sum_{c_n=0}^{\infty} P(\lambda ; \overrightarrow{c}) ~ P(\overrightarrow{c} ; \overrightarrow{s})$ \\
	\end{tabular}
\end{equation}
\noindent where
\begin{equation*}
	\begin{tabular}{r@{ = }l}
		$P(\lambda ; \overrightarrow{c})$ & $Gam\Bigg(\lambda ; \displaystyle\sum_{i=1}^{n} x_i + \alpha, n + \beta \Bigg)$
	\end{tabular}
\end{equation*}
\noindent with $\overrightarrow{c} = (c_1, \ldots, c_n)$ for $1 \leq i \leq n$.

% $P(\overrightarrow{x} \mid \overrightarrow{s})$ is factored based on the assumption that each sensor is conditionally independent of the other sensors given $x_i$. Consequently, the probability that the vector of true counts is $\overrightarrow{x}$, given $n$ samples of the vector of $m$ sensed counts $\overrightarrow{s_1}, \ldots, \overrightarrow{s_n}$, is
% 
% \begin{equation}
%     \label{eq:occurrences_likelihood}
%     \begin{tabular}{r@{ $\varpropto$ }l}
%         $P(\overrightarrow{x} \mid \overrightarrow{s_1}, \ldots, \overrightarrow{s_n})$ & $P(\overrightarrow{s_1}, \ldots, \overrightarrow{s_n} \mid \overrightarrow{x}) ~ P(\overrightarrow{x})$ \\ [1ex]
%         & $\displaystyle\prod_{i=1}^{n} P(\overrightarrow{s_i} \mid x_i) ~ P(x_i)$ \\ [2ex]
%         & $\displaystyle\prod_{i=1}^{n} \displaystyle\prod_{j=1}^{m} P(s_{ji} \mid x_i) ~ P(x_i \mid \overrightarrow{x_{-1}})$
%     \end{tabular}
% \end{equation}
% 
% \noindent where $\overrightarrow{x_{-1}} = x_{i-1}, \ldots, x_1$.
% 
% $P(x_i \mid \overrightarrow{x_{-1}})$ is calculated in the form of a negative binomial distribution
% 
% \begin{equation}
% 	\label{eq:unconditional_xi}
% 	\begin{tabular}{r@{=}l}
% 		$P(x_i \mid \overrightarrow{x_{-1}})$ & $\displaystyle\int_{\lambda=0}^{\infty} P(x_i \mid \lambda) ~ P(\lambda \mid \overrightarrow{x_{-1}}) ~d\lambda$ \\ [2ex]
% 		& $\displaystyle\int_{\lambda=0}^{\infty} Poi(x_i \mid \lambda) ~ Gam(\lambda \mid \alpha, \beta) ~d\lambda$ \\ [2ex]
% 		& $NB\Bigg(x_i \mid \alpha, \displaystyle\frac{\beta}{\beta + 1}\Bigg)$
% 	\end{tabular}
% \end{equation}
% 
% \noindent and $P(s_{ji} \mid x_i)$ is defined as the aggregate of the true positives $tp_{ji}$ in $x_i$ sub-intervals, and the false positives $fp_{ji}$ in $l-x_i$ sub-intervals. The probability of a TP for sensor $j$ in a sub-interval is $tpr_j = P_j(d = 1 \mid e=1)$, and the probability of an FP is $fpr_j = P_j(d = 1 \mid e=0)$. Thus $P(s_{ji} \mid x_i)$ is defined as a sum of two binomial distributions $B(r \mid n,\pi)$, where the aggregate is constrained to be $s_{ji}$: 
% 
% \begin{equation}
% 	\label{eq:joint_binomial_distribution}
%     P(s_{ji} \mid x_i) \! = \! \! \! \displaystyle\sum_{\textrm{tp}_{ji} = 0}^{x_{i}} \! \! B\Big(\textrm{tp}_{ji} \mid x_i, \textrm{tpr}_j\Big) B\Big(\textrm{fp}_{ji} \mid \Delta x_i, \textrm{fpr}_j \Big)
% \end{equation}
% \noindent where $s_{ji} = \textrm{tp}_{ji} + \textrm{fp}_{ji}$, $\textrm{tpr}_j = P_j(d=1 \mid e=1)$, $\textrm{fpr}_j = P_j(d=1 \mid e=0)$, and $\Delta x_i = (l - x_i)$.
% 
% Eqn.~\ref{eq:marginal_occurrences} shows the difficulty of belief state estimation in a POPP since there is no conjugate density. Each sensed count $\overrightarrow{s_i}$ sample used to update the posterior of $\lambda$ adds a factor of countably infinite number of elements. The resulting posterior is a sum of countably infinite sums. One can place an upper bound $l$ on the maximum value of each $x_i$, but it still makes the number of elements in the posterior grow by a factor $l$ with every sensed count $\overrightarrow{s_i}$.  
% 
% With this difficulty noted, Jovan et al., proposed three efficient estimators, each of which offers an approximation to the true posterior $P(\lambda \mid \overrightarrow{s})$. A more detailed presentation of these estimators is given in \cite{jovan18a}.
